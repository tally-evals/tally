---
alwaysApply: true
---
## ðŸ§  Mental Model

Tally enforces **composability by value**â€”you compose evaluations by wiring actual objects (MetricDefs, scorers, aggregators), not by juggling string identifiers.

| Concept        | Analogy             | Purpose                                                                 |
| -------------- | ------------------- | ----------------------------------------------------------------------- |
| **Tally**      | Evaluation Kit      | Complete container with everything you need                             |
| **MetricDef**  | Ruler & Blueprint   | Defines what to measure (LLM prompt/code) and is reused as a value      |
| **Metric**     | Measurement         | Actual numeric or boolean result                                        |
| **Scorer**     | Normalizer/Combiner | Applies MetricDefs to produce a normalized 0â€“1 derived metric object    |
| **Evaluator**  | Orchestrator        | Manages measurement flow by combining MetricDefs, scorers, and execution context|
| **Aggregator** | Statistician        | Summarizes derived metrics supplied directly by scorers                 |
| **Report**     | Scorecard           | Final output for analysis and visualization                             |

---

## ðŸ”— Data Flow

```
Tally<T> Container
    â†“
Data (Dataset/Conversation)
    â†“
MetricDefs â†’ Raw Metrics (Generated by Code/LLM)
    â†“
EvaluationContext (single-turn run policy) â†’ selects targets for single-turn metrics
    â†“
Resolve Scoring Context (per MetricDef)
    â†“
Normalize Raw Metrics â†’ Scores (0â€“1)
    â†“
Evaluator â†’ Executes Scorer
    â†“
Scorer â†’ Combines Scores â†’ Derived Composite Metric (Score 0â€“1)
    â†“
Aggregators â†’ Summaries
    â†“
Report (Per-target + Aggregate Results)
```

The **Tally Orchestration**: The container manages the entire flow internally â€” you just provide the data and configuration, and Tally handles the rest. Single-turn metrics respect the run policy supplied through the EvaluationContext, while multi-turn metrics are always evaluated against the complete conversation.

---

## ðŸ“Š Reports - The Visual Showcase of Runs

Reports are the final output from running a `Tally<T>` evaluation. They contain the results in a structured format:

### What Reports Contain

* **Per-target Results:** Individual measurements for each data point (both raw and composite metrics)
* **Aggregate Summaries:** Overall statistics across all data

### The Visual Showcase

Reports serve as the final, presentable output, providing a comprehensive visual showcase of evaluation performance. They are designed for easy consumption, analysis, and, critically, visual comparison of metrics across different Tally runs, model versions, or configurations.


