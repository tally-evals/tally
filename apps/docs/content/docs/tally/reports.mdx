---
title: Reports
description: The comprehensive source of truth for evaluation runs.
---

The **Evaluation Report** is the final output of the Tally engine. It is a structured, serializable "Source of Truth" that captures every measurement, every decision (verdict), and every summary from an evaluation run.

A report is designed to serve two audiences:
1.  **Stakeholders**: Through high-level aggregated summaries.
2.  **Developers**: Through granular, turn-by-turn failure analysis.

---

## Anatomy of a Report

When you call `await tally.run()`, you receive a report object with three distinct sections:

### 1. Eval Summaries (`evalSummaries`)
The bird's-eye view. This section contains the output of your [Aggregators](/docs/tally/aggregators). It tells you how each Eval performed across the entire dataset.

- **Example**: `Relevance: 85% Pass Rate, 0.78 Mean Score`.
- **Single-turn vs multi-turn**: each entry includes `evalKind: 'singleTurn' | 'multiTurn' | 'scorer'`.

### 2. Per-Target Results (`perTargetResults`)
The deep-dive data. For every single conversation or step in your dataset, Tally records:
- The raw metric value (e.g., `500ms`).
- The normalized score (e.g., `0.8`).
- The final verdict (`pass` or `fail`).
- Any error messages if the evaluation failed.

**How to differentiate single-turn vs multi-turn here:** `perTargetResults` is “one result per target container”. If you ran on a dataset of single-turn items, each entry is a turn/item. If you ran on a dataset of conversations, each entry is a conversation (multi-turn).

### 3. Metadata (`metadata`)
Contextual information about the run, including:
- Total execution time.
- Timestamp of the run.
- Model names and versions used for evaluation.

---

## Visibility and Debugging

Tally makes it easy to visualize these reports wherever you work.

### CLI Tables
For a quick check during development, Tally can format the report as a clean table in your terminal.

```ts
import { formatReportAsTables } from '@tally-evals/tally';

const report = await tally.run();
console.log(formatReportAsTables(report));
```

### Failure Analysis
Because the report contains the full context for every `fail` verdict, you can quickly identify and fix the specific interactions that are dragging down your scores.

```ts
const failures = report.perTargetResults.filter((r) => {
  for (const [, v] of r.verdicts) {
    if (v.verdict === 'fail') return true;
  }
  return false;
});

failures.forEach(f => {
  console.log(`Failure in ${f.targetId}:`, f.verdicts);
});
```

---

## CI/CD Integration: "Fail Fast"

Tally reports are first-class citizens in your automation pipeline. By parsing the `evalSummaries`, you can automatically block problematic code before it reaches production.

```ts
const report = await tally.run();
const relevance = report.evalSummaries.get('Answer Relevance');
if (!relevance) throw new Error('Missing summary');

const passRate = relevance.aggregations.passRate;
if (passRate !== undefined && passRate < 0.95) {
  throw new Error(`Evaluation Failed: Pass rate (${passRate}) is below 95%`);
}
```

If you want to split summaries by single-turn vs multi-turn:

```ts
for (const [evalName, summary] of report.evalSummaries) {
  if (summary.evalKind === 'multiTurn') {
    console.log('multi-turn', evalName, summary.aggregations.mean);
  }
}
```

---

## Summary

The report is not just an output; it is a **historical record** of your system's quality. By saving these reports over time, you build a comprehensive map of your agent's evolution and performance.
